<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AGI on Lyuwen Fu</title><link>https://lyuwenfu.me/tags/agi/</link><description>Recent content in AGI on Lyuwen Fu</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 20 Aug 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://lyuwenfu.me/tags/agi/index.xml" rel="self" type="application/rss+xml"/><item><title>Finding efficiency gains in data loading procedure in Megatron Core framework</title><link>https://lyuwenfu.me/posts/megatron-core/</link><pubDate>Tue, 20 Aug 2024 00:00:00 +0000</pubDate><guid>https://lyuwenfu.me/posts/megatron-core/</guid><description>It supprises me that I am able to extract over hundred times efficiency gains in the SOTA LLM framework Megatron Core, but it happened. Here are the details.
Distributed dataset builder The original algorithm in Megatron Core only attempts to build the dataset indices cache at rank 0 with torch.multiprocessing parallel, and than let the rest of the ranks read the cached data. But at large scale training with hundreds, or even thousands of GPUs, one has even more CPUs at disposal simultaneously.</description></item></channel></rss>