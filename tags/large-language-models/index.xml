<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Large Language Models on Lyuwen Fu</title><link>https://lyuwenfu.me/tags/large-language-models/</link><description>Recent content in Large Language Models on Lyuwen Fu</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 20 Aug 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://lyuwenfu.me/tags/large-language-models/index.xml" rel="self" type="application/rss+xml"/><item><title>Finding efficiency gains in data loading procedure in Megatron Core framework</title><link>https://lyuwenfu.me/posts/megatron-core/</link><pubDate>Tue, 20 Aug 2024 00:00:00 +0000</pubDate><guid>https://lyuwenfu.me/posts/megatron-core/</guid><description>&lt;p&gt;It supprises me that I am able to extract over hundred times efficiency gains in the SOTA LLM framework Megatron Core, but it happened. Here are the details.&lt;/p&gt;
&lt;h4 id="distributed-dataset-builder"&gt;
 Distributed dataset builder
 &lt;a class="heading-link" href="#distributed-dataset-builder"&gt;
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"&gt;&lt;/i&gt;
 &lt;span class="sr-only"&gt;Link to heading&lt;/span&gt;
 &lt;/a&gt;
&lt;/h4&gt;
&lt;p&gt;The original algorithm in Megatron Core only attempts to build the dataset indices cache at rank 0 with &lt;code&gt;torch.multiprocessing&lt;/code&gt; parallel,
and than let the rest of the ranks read the cached data.
But at large scale training with hundreds, or even thousands of GPUs, one has even more CPUs at disposal simultaneously.
Assuming a reasonably high performance parallel file system, the IO cost of reading/writing cache files is negligible.
Thus, it&amp;rsquo;s inherently more efficiently to distribute the building processing to the whole cluster and then read the entire cache back
for all ranks.&lt;/p&gt;</description></item></channel></rss>